{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise as target\n",
    "A keras implementation of the [Unsupervised learning by predicting noise](https://arxiv.org/abs/1704.05310) paper  \n",
    "  \n",
    "Here's an analysis of the paper I rather enjoyed. [link](http://www.inference.vc/unsupervised-learning-by-predicting-noise-an-information-maximization-view-2/)  \n",
    "  \n",
    "This is an extraction and reformatting of the implementation my team and I made as part of the \"[Mozgalo](https://www.estudent.hr/category/natjecanja/mozgalo/)\" competition.  \n",
    "  \n",
    "I have the [celebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) dataset on hand, and it's a decently big and publicly available dataset unlike mozgalo so this demo uses it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.pooling import MaxPooling2D, GlobalAveragePooling2D\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "from itertools import cycle, islice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_shape = (224,224)\n",
    "batch_size = 128\n",
    "n_features = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "This is a [darknet](https://pjreddie.com/darknet/imagenet/) model.  \n",
    "You can use whatever you wish. One of the things I find so nice about this method is that the models are interchangeable with standard classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convolutional_block(inp, filters):\n",
    "    x = Conv2D(filters=filters, kernel_size=3, strides=(1,1), padding='same', use_bias=False)(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = MaxPooling2D(pool_size=(2,2), strides=2)(x)\n",
    "    return x\n",
    "\n",
    "c = 1  # we train on b&w images, as the paper suggests, to make the training objective harder \n",
    "\n",
    "\n",
    "input_img = Input(shape=(*image_shape, c))\n",
    "\n",
    "x = convolutional_block(input_img, 16)\n",
    "\n",
    "x = convolutional_block(x, 32)\n",
    "\n",
    "x = convolutional_block(x, 64)\n",
    "\n",
    "x = convolutional_block(x, 128)\n",
    "\n",
    "x = convolutional_block(x, 256)\n",
    "\n",
    "x = convolutional_block(x, 512)\n",
    "\n",
    "x = convolutional_block(x, 1024)\n",
    "\n",
    "x = Conv2D(filters=n_features, kernel_size=1, strides=(1,1), padding='same', use_bias=False)(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "model = Model(inputs=input_img, outputs=x)\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam')\n",
    "\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### or if you prefer, load a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model('../models/something')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rand_unit_sphere(npoints, ndim=1000):\n",
    "    '''\n",
    "    Generates \"npoints\" number of vectors of size \"ndim\"\n",
    "    such that each vectors is a point on an \"ndim\" dimensional sphere\n",
    "    that is, so that each vector is of distance 1 from the center\n",
    "    \n",
    "    npoints -- number of feature vectors to generate\n",
    "    ndim -- how many features per vector\n",
    "    \n",
    "    returns -- np array of shape (npoints, ndim), dtype=float64\n",
    "    '''\n",
    "    vec = np.random.randn(npoints, ndim)\n",
    "    vec = np.divide(vec, np.expand_dims(np.linalg.norm(vec, axis=1), axis=1))\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noises_from_keys(noises, keys):\n",
    "    '''\n",
    "    returns the assigned noise vectors for a batch of images\n",
    "    '''\n",
    "    keys = list(keys)\n",
    "    n = np.empty(shape=(len(keys), n_features), dtype='float64')\n",
    "    for i in range(len(keys)):\n",
    "        n[i] = noises[keys[i]]\n",
    "        \n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_assigned_noises(noises):\n",
    "    '''\n",
    "    shuffles all of the noises assigned to images\n",
    "    done evety N epoch to avoid plateaus\n",
    "    '''\n",
    "    new_noises = {}\n",
    "    keys = noises.keys()\n",
    "    values = list(noises.values())\n",
    "    np.random.shuffle(values)\n",
    "    \n",
    "    for k, v in zip(keys, values):\n",
    "        new_noises[k] = v\n",
    "        \n",
    "    return new_noises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def datagen_wrapper(datagen, noises):\n",
    "    filename_iter = islice(cycle(datagen.filenames), datagen.batch_size)\n",
    "    \n",
    "    while True:\n",
    "        ns = noises_from_keys(noises, filename_iter.__next__())\n",
    "        yield datagen.__next__(), ns\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 202599 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "            rotation_range=40,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            rescale=1./255,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest')\n",
    "\n",
    "datagen = datagen.flow_from_directory(\n",
    "    '../data/',\n",
    "    target_size=image_shape,\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    shuffle=False,\n",
    "    color_mode='grayscale'\n",
    "    )\n",
    "\n",
    "n_images = datagen.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unit_sphere_noises = rand_unit_sphere(n_images, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assigned_noises = {}\n",
    "\n",
    "for fn, ft in zip(datagen.filenames, unit_sphere_noises):\n",
    "    assigned_noises[fn] = ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datagen_wrapper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-82e30c735ec9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatagen_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatagen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massigned_noises\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'datagen_wrapper' is not defined"
     ]
    }
   ],
   "source": [
    "gen = datagen_wrapper(datagen, assigned_noises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'm', 'g', '_', 'a', 'l', 'i', 'g', 'n', '_', 'c', 'e', 'l', 'e', 'b', 'a', '/', '1', '1', '2', '4', '2', '3', '.', 'j', 'p', 'g']\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-e70140de4dfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#gen_wrapper(gen, filenames, noises)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-e86f87bedec7>\u001b[0m in \u001b[0;36mdatagen_wrapper\u001b[0;34m(datagen, noises)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnoises_from_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoises\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename_iter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mdatagen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-1a357dc4fdb2>\u001b[0m in \u001b[0;36mnoises_from_keys\u001b[0;34m(noises, keys)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float64'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnoises\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "x, y = gen.__next__()\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "#gen_wrapper(gen, filenames, noises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(cycle(datagen.filenames))[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs_per_shuffle=3\n",
    "\n",
    "# base name of model save file - based on training start time\n",
    "save_dir = '../models/'\n",
    "folder_name = time.strftime(\"%Y-%m-%d-%H-%M/\", time.gmtime())\n",
    "save_dir = save_dir + folder_name\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "epoch = 1\n",
    "while True:\n",
    "    print('Running epoch:', epoch)\n",
    "    for b in range(3):\n",
    "        for i in tqdm(n_images / batch_size))):\n",
    "            # get image batch\n",
    "            images, keys = batch_generator.__next__()\n",
    "            # get noise mapped to each image in batch\n",
    "            n = noises_from_keys(keys)\n",
    "\n",
    "            # do forward pass on batch - features\n",
    "            losses = np.empty(shape=(batch_size, batch_size), dtype='float64')\n",
    "            features = model.predict_on_batch(images)\n",
    "            \n",
    "            # calculate l2 loss between all...\n",
    "            # noises randomly assigned and features generated by the network\n",
    "            for b in range(batch_size):\n",
    "                fts = np.repeat(np.expand_dims(features[b], axis = 0), batch_size, axis=0)\n",
    "                l2 = np.linalg.norm(fts - n, axis=1)\n",
    "                losses[b] = l2\n",
    "            \n",
    "            # rearrange noises such that the total loss is minimal (hungarian algorithm)\n",
    "            row_ind, col_ind = linear_sum_assignment(losses)\n",
    "            for r, c in zip(row_ind, col_ind):\n",
    "                noises[keys[r]] = n[c]\n",
    "                \n",
    "            n = noises_from_keys(keys)\n",
    "            \n",
    "            # do a backprop pass with the newly arranged features\n",
    "            model.train_on_batch(images, n)\n",
    "            \n",
    "        # every epoch save model\n",
    "        model.save(save_dir  + str(epoch) + '.model')\n",
    "        epoch += 1\n",
    "            \n",
    "    # every cca 3 epochs shuffle noises\n",
    "    noises = shuffle_noises(noises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
